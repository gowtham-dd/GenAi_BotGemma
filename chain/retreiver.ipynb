{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Detection of Bacterial Signatures in Genomic DNA Sequences using Hybrid \\nML-DL Pipeline \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbstract \\nProper identification and classification of bacterial life \\nin DNA sequences is one of the central roles of \\nbioinformatics with general implications for clinical \\ndiagnosis, typing of pathogens, characterization of \\nmicrobiome, and genetic research. A fast pace of \\nadvancements in sequencing technologies has amassed \\nvast volumes of genomic data, necessitating robust \\ncomputational approaches to analyze and interpret \\nbiological patterns effectively.Although earlier \\nmachine learning (ML) and deep learning (DL) \\nmodels have been suggested for the task, a more \\ncomprehensive comparative pipeline of both classic \\nML models and DL architectures has not been \\ndeveloped hitherto in this work. The ML approach \\nbegins with rigorous feature engineering, where we \\nobtain biologically meaningful descriptors such as \\nnucleotide composition, k-mer frequency distributions, \\nShannon entropy, and sequence complexity measures. \\nThese features are then employed for the training of \\nthree popular classifiers: Random Forest (RF), \\nSupport Vector Machine (SVM), and Extreme \\nGradient Boosting (XGBoost).At the same time, we \\ndevelop a Convolutional Neural Network (CNN) \\nmodel that processes raw integer-encoded DNA \\nsequences directly without any explicit feature \\nextraction. The CNN model extracts local patterns and \\ndistant dependencies in sequences such that the model \\nlearns deep representations.Our results indicate that all \\nmodels are good at generalizing, with CNN \\ngeneralizing and performing just a little better \\ncompared to the traditional classifiers in terms of noise \\nrobustness. However, the ML models provide better \\ninterpretability, particularly in terms of feature \\nimportance and biological relevance. \\nThis combined system demonstrates the power of \\nbringing together interpretable machine learning and \\nthe automatic feature learning capabilities of deep \\nlearning, paving the way towards more accurate and \\ninterpretable bacterial DNA classification systems. \\nKeywords: DNA sequence classification, bacterial \\ndetection, bioinformatics, machine learning, deep \\nlearning, convolutional neural network, feature \\nengineering, k-mer frequency, Shannon entropy, \\nRandom Forest, SVM, XGBoost, genomic analysis, \\nexplainable AI. \\nGowtham D  \\nIII M.Sc SS  \\nDepartment of Software Systems And Aiml \\n Sri Krishna Arts And Science College  \\nCoimbatore, India  \\ngowthamd22mss011@skasc.ac.in \\n \\nProf. Sabeena S  \\nDepartment of Software Systems And Aiml \\n Sri Krishna Arts And Science College  \\nCoimbatore, India  \\nsabeenas@skasc.ac.in'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='I. Introduction \\nDNA sequence classification is the ab initio of \\nthe core modern computational biology, helping \\nrecognize a bacterial and viral infection, identify \\ngenetic mutation, and detect certain pathogenic \\nmarkers. All these processes are the very \\nfoundation of disease diagnosis, antimicrobial \\nresistance prediction, personalized medicine, and \\nmicrobiome analysis. As high-throughput \\nsequencing-related technologies evolve in time, \\nthey have to ingest oceanic volumes of genomic \\ndata, thus rendering manual analysis techniques \\ngigantic and slow. \\nTo mitigate these challenges, computational \\nmeans are being deployed to classify DNA \\nsequences automatically. Traditional classical \\nML-based methods-Random forest, SVM, and \\nGradient Boosting-have been shown to work \\nwell for genomics studies. Such models cater to \\nengineered features like nucleotide composition, \\nk-mer frequency profiles, GC content, and \\nentropy-based features that statistically and \\nstructurally characterize DNA. \\nWhile successful, these ML models are often in \\nneed of handcrafted feature design and tend to \\ndisregard more subtle sequence patterns in the \\nDNA profile. Great DL breakthroughs that \\ncurrently prevail particularly in convolutional \\n \\nII. Related Work  \\n \\nDNA sequence classification employs \\ncomputational methods to be used in \\nbioinformatics for taxonomic classification of an \\norganism, identification of pathogens, or for \\nmetagenomic analysis. The majority of the \\nexisting work employs k-mer-based methods, \\nwherein smaller DNA substrings of a fixed size \\nare extracted from DNA sequences and then used \\nas features in traditional classification schemes. \\nManual features, when used with Naïve Bayes, \\nRandom Forests, and Support Vector Machines, \\nyield the best results. Kraken and CLARK \\nclassify sequences through high-speed exact k-\\nmer searching against reference databases. Both \\nthe methods provide the greatest speed and \\naccuracy with familiar organisms but do not have \\ngreat prowess in unknown taxa because of heavy \\ndependency on reference genomes and \\nincapability of flexing toward some new \\ngenomic patterns. With the emergence of deep \\nlearning, the new possibilities arose for sequence \\nclassification. The RNNs and CNNs \\nautomatically learn hierarchical features from \\nraw RNA or DNA sequences without requiring'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='manual feature engineering. Thus, an example \\nwould be DeepSEI \\nIII. Dataset Description \\nThe data in use are from a Kaggle competition \\ntitled \"3722genomics\", which classifies DNA \\nsequences for the identification of a bacterial \\npresence. Each sample in the data carries its \\nunique id, a DNA sequence, and a binary tag (Y) \\nin which 1 indicates bacterial presence while 0 \\nmeans bacterial absence. The data are distributed \\ninto a labeled training set and an unlabeled test \\nset containing 100,000 and 20,000 sequences \\nrespectively. \\nThe class balance in the training dataset is nearly \\neven, with 50,063 of the negative (no bacteria) \\nclass instances and 49,937 of the positive (where \\nbacteria are present) class instances. The \\nbalanced representation prevents the model from \\nfavoring one class over another and equally \\naccounts for performance measurement. \\nDNA sequences vary in length in this data set, \\nthus replicating an actual situation within \\ngenomic classification problems. Sequence \\nlength is a random variable between 121 and 425 \\nbase pairs. The distribution of lengths is \\npresented with a mean of roughly 268 base pairs \\nand a standard deviation of 77.08. The \\ninterquartile range shows that 25% of the \\nsequences are shorter than 202 base pairs, while \\n75% \\nFigure 1: Count Plot for Target Distribution. \\n \\n \\nFigure 2: DNA Sequence Distribution. \\n \\nIV. Methodology \\n4.3.1 Random Forest Classifier \\nRandom Forest is an ensemble learning approach \\nbased on decision trees. Its process begins by \\ntraining a considerable amount of trees \\nsimultaneously, and when classifying it takes a \\nmajority vote of all the trees. Random Forest is a \\nrobust, non-parametric machine learning \\napproach suitable for high dimensional feature \\nspaces with relatively low risk of overfitting \\nwhen applying noisey bio data.  \\nRandom Forest was selected due to the \\ninterpretable nature of its predictions, and the \\ninherent feature importance score that'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='complements our process for selecting variables. \\nRandom Forest used the default hyperparameters \\nand was implemented with 100 estimators \\nTable 1. Performance metrics of RF \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.97 0.98 9,987 \\n \\nFigure 3: Confusion matrix for Random Forest \\nclassifier. \\n \\n \\n4.3.2 Support Vector Machine (SVM) \\nSupport Vector Machines depend on finding the \\nhyperplane that optimally separates the data into \\nclasses, a non-linear example of which can be \\naddressed using kernel tricks, such as RBF, \\nwhich will map the input features into higher \\ndimensions where the data is linearly separable. \\nSVM is most powerful in high-dimensions, and \\nit is also very well known for its properties of \\ngeneralization. \\nSince we are dealing with features from DNA \\nsequences, which will likely be non-linearly \\nseparable, we applied the RBF kernel in our \\ntrials. The regularization parameter C was set to \\nits default value in order to balance having a \\nsmall error on the training data and a large \\nmargin. \\n \\n \\nTable 2. Performance metrics of SVM \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.98 0.98 9,987 \\n \\nFigure 4: Confusion matrix for Support Vector \\nMachine.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='4.3.3 XGBoost Classifier \\nXGBoost or Extreme Gradient Boosting is a very \\nfast regularized boosting algorithm that performs \\nvery well on structured data problems. XGBoost \\nbuilds decision trees sequentially, so one tree \\nshould help minimize the errors from the \\nprevious trees. It also has regularization terms in \\nthe objective function to avoid overfitting.  \\nWe chose XGBoost because it can also \\nefficiently work on sparse high-dimensional \\ndata, and it also has built-in parallelization in the \\ntraining which makes computation faster. Some \\nof the parameters we used were max_depth=6, \\nn_estimators=100 and learning_rate=0.1. \\nTable 3. Performance metrics of XGB \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.98 0.98 9,987 \\n \\nFigure 5: Confusion matrix for XGBoost \\nclassifier. \\n \\n \\n4.3.4 Model comparison and takeaways \\nAll models performed nearly the same with 98% \\noverall accuracy, suggesting biological inspired \\nfeatures are good predictors. Although SVM and \\nXGBoost had slightly higher precision and recall \\nthan Random Forest those differences were \\nnegligible. All confusion matrices among the \\nmodels suggest balanced classification with very \\nlittle misclassification which shows the feature \\nselection and preprocessing pipeline were best \\nmatched to the classification problem.  \\nA radar plot or side by side comparative bar \\nchart of the precision, recall and F1-score for all \\nmodels could be included to view these metrics \\nin a side by side manner. \\nFigure 6: Model performance comparison \\nacross Random Forest, SVM, and XGBoost.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content=\"4.3 Deep Learning with 1D Convolutional \\nNeural Network (CNN) \\nTo eliminate manual feature extraction and allow \\nthe model to learn directly from the genomic \\nsequences in its raw form, a 1D Convolutional \\nNeural Network (CNN) was used. This structure \\nis particularly appropriate for sequential data and \\nhas performed exceedingly well in applications \\ninvolving natural language and biological \\nsequences. The key advantage of CNNs is that \\nthey perform localized pattern and motif finding \\nthrough hierarchical feature processing. \\nSequence Encoding and Preprocessing \\nAll DNA sequences in the dataset are made up of \\ncombinations of the four nucleotide bases: \\nAdenine (A), Cytosine (C), Guanine (G), and \\nThymine (T). Initially, they were encoded as \\nintegers: A = 0, C = 1, G = 2, T = 3. Because \\nsequences vary in length in the dataset, the \\nshorter sequences were padded with a specific \\ninteger token (PAD = 4) to a uniform length. \\nSequences were padded or cut-off from their end \\nto the maximum length of 425 bases depending \\non their lengths' statistical distributions. The \\npreprocessing did ensure uniformity but had no \\nadverse affect on biologically important \\ninformation. \\nCNN Model Architecture \\nThe CNN model was developed to learn high-\\nlevel features from the integer-encoded genomic \\nDNA sequences. This architecture has several \\nlayers: \\n•Embedding Layer: This layer converts the \\ninteger encodings of the nucleotide bases to \\ndense vector embeddings. A base embedding \\nbecomes a learnable embedding, and thus the \\nmodel learns the semantic similarity of \\nnucleotides in the context of the genome. \\n•Convolutional Layers: Several 1D \\nconvolutional layers of different kernel sizes \\n(i.e., 5, 7) to learn local patterns or motifs in the \\nsequence. These motifs will typically denote \\nfunctional sites in the DNA that have meaningful \\nroles in classification. \\n•Max-Pooling Layers: Following every \\nconvolution block is a max-pooling layer to \\ndecrease the dimensions of the feature maps. \\nThis not only maximizes computation efficiency, \\nbut allows for spatial invariance; keeping only \\nthe most salient features, thus preserving only \\nnecessary information. \\n•Flatten Layer: The output of the last pooling \\noperation is then flattened into a one-\\ndimensional vector for classification utilizing \\ndense layers.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='•Dense Layers with Dropout: Fully connected \\nlayers one or more are used, with ReLU \\nactivations to build in non-linearity. Dropout \\nregularization is used to reduce overfitting by \\nrandomly disabling a number of neurons during \\ntraining.  \\n•Output Layer: The output dense layer uses a \\nsigmoid activation in order to produce \\nprobability scores for binary classification or, \\nmore specifically, the predicted probability of \\nbacteria present (label = 1).  \\n•Optimization and Loss: The model is \\noptimized with the binary cross entropy loss \\nfunction and the adam optimizer. Adams \\nadaptive learning rate allows it to converge more \\nquickly and generalize better.  \\nTraining Strategy and Monitoring \\nThe dataset was split into a training set and a \\nvalidation set, which the model performed \\nevaluation on while training. The validation \\naccuracy and loss were recorded over the epochs \\nof training, and an early stopping criterion was \\nadded to allow the model to stop training when it \\nstopped improving. This will help prevent \\noverfitting and allow the model to generalize to \\nunseen test data better. \\n \\n \\n \\n \\nFigure 7: Architecture of the 1D Convolutional \\nNeural Network used for genomic sequence \\nclassification.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Model Evaluation \\nThe CNN was evaluated on the test set, which \\ncontained 20 thousand separate unseen DNA \\nsequences. The results indicated a robust level of \\nclassification performance, achieving high scores \\n(precision, recall and F1-scores) across both \\nclasses. Below is the full classification report: \\nTable 4. Performance metrics of CNN \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n0 0.98 0.99 0.98 10,013 \\n1 0.99 0.98 0.98 9,987 \\nOverall these results show that the model is able \\nto appropriately discriminate between samples \\nthat are bacterial and non-bacterial, performing \\nsimilarly across both classes. \\nFigure 8: Confusion matrix showing true and \\npredicted labels for the CNN mode. \\n \\nThe confusion matrix suggests that the \\nmislabeled instances are relatively small, and the \\nmajority of the predictions were correctly \\naligned with the ground truth labels. \\n \\nV. Results \\n5.1 Evaluation Metrics \\nFor evaluating the performance of our models, \\nwe used standard classification metrics: \\naccuracy, precision, recall, and F1-score. All \\nevaluations were also completed on a stratified \\nvalidation set to ensure a balanced representation \\nof each class. \\nThe Random Forest and XGBoost classifiers \\nperformed similarly well, both with F1-scores \\nover 0.98, with XGBoost doing slightly better in \\nprecision and recall measures. The Support \\nVector Machine (SVM) model performed \\nadequately, but had generally lower outcomes \\nthan the other classifiers with regards to F1- \\nscores and accuracy. \\nThe best performing 1D Convolutional Neural \\nNetwork (CNN) architecture yielded the best F1-\\nscore of 0.989, with excellent generalization \\ndemonstrated across the complete dataset, as \\ndemonstrated through macro and weighted \\naverage metrics. These results suggest that deep \\nlearning models exhibit marginally better \\npredictive performance than traditional machine \\nlearning models when tasked with DNA \\nsequence classification tasks, albeit at the cost of \\ninterpretability.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Table 5. Performance metrics comparison \\nacross models \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\nRF 0.981 0.982 0.981 0.981 \\nXGB 0.984 0.985 0.983 0.984 \\nSVM 0.974 0.976 0.973 0.974 \\nCNN \\n(1D) \\n0.989 0.99 0.988 0.989 \\n \\n5.2 Visualization and Interpretability \\nTo improve interpretation and to facilitate \\nperformance comparison between models, we \\nwill plot some key evaluation areas: \\n• A bar plot of F1-scores for all the models for a \\nrelative comparison of model performance. \\n• A plot of feature importance of the Random \\nForest model that includes the top 10 \\ncontributing features. These features are high-\\nfrequency k-mers and compositional features \\nsuch as GC content that have biological meaning \\nin taxonomic classification. \\n• The training history charts for the CNN model \\n(training and validation accuracy and loss vs \\nepochs), confirm that the model converged \\nwithout overfitting (i.e., demonstrating strong \\nlearning with the raw DNA sequences). \\nFigure 9: F1-score comparison across ML and \\nDL models. \\n \\nFigure 10: Feature importance plot of top 10 \\nfeatures from Random Forest model. \\n \\nFigure 11: CNN training history showing \\naccuracy and loss over epochs.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content=\"5.3 Model Insights and Comparative Analysis \\nThe evaluation offers significant insights into the \\nadvantages and trade-offs of all approaches: \\n• CNN models are very good at examining \\nsequence motifs directly from raw DNA, with no \\nfeature engineering necessary by a human. The \\nmodel's overall best, especially on bigger \\ndatasets, demonstrates a good ability to identify \\ncomplex biological patterns. \\n• Machine Learning models, while slightly \\ntrailing in raw performance, have significantly \\nbetter interpretability and computational cost. \\nThe feature importance outputs of Random \\nForest and XGBoost not only provide \\nexplainability but also begin to show us how to \\nthink about the sequence patterns at a biological \\nlevel- like how GC-rich motifs are usually in the \\ngenomes of microbes. \\n• This hybrid evaluation also emphasizes the \\nneed for the combination of ML explainability \\nwith DL accuracy, especially in fields where \\nperformance and explainability are most \\nimportant. \\nOverall, our comparative work illustrates that \\ntraditional and deep learning methods have \\ninherent strengths to be considered and ought to \\nbe chosen based on the demands of the specific \\nbiological application. \\nVI. Discussion \\nThe results of this work demonstrate the \\ncomplementary strengths of the state-of-the-art \\nMachine Learning (ML) and Deep Learning \\n(DL) approaches, for the case of bacterial \\nclassification from genomic data. The \\nperformance of traditional ML models, i.e., \\nRandom Forest, XGBoost and Support Vector \\nMachines, is reasonable with hand-engineered \\nfeatures such as k-mer frequencies and \\ncompositional statistics. Traditional models have \\nadvantages of low computational overhead, fast \\ninference, and good interpretability making them \\napplicable in real-time or low-resource \\ndeployment. In contrast, our 1D Convolutional \\nNeural Network (CNN) offered modestly better \\npredictive accuracy and F1-score compared to \\ntraditional models, and could learn from raw \\nDNA sequence data. CNNs are capable of \\nidentifying subtle, complex, non-linear motifs, \\nthat may be difficult to hand-engineer, and are \\nparticularly useful in noisy or heterogeneous \\ngenomic data. But come with greater \\ncomputational overhead, longer training time and \\nweaker interpretability.  For many use cases such \\nas large-scale genomic surveillance, \\nmetagenomic classification or auto-diagnosis, \\nDL approaches offer improved scalability and\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='flexibility to deal with unknown data. \\nOne of the promising avenues is hybrid or \\nensemble approaches, which leverage the \\ninterpretability of ML and combine it with the \\nDL representation learning capability. The \\nensembles can apply voting or stacking methods \\nto further improve classification accuracy with \\ntransparency from the model. \\nVII. Conclusion  \\nWe report here a complete pipeline for \\nclassifying bacteria by using DNA sequences in \\nconcert with feature-engineering machine \\nlearning models, and end-to-end deep learning \\nmodels.  \\nOur approach performs very well for \\nclassification, with all models producing \\nclassification F1-scores > 0.97 and the CNN \\nproducing an F1-score of 0.989, demonstrating \\nthat it is possible to automate microbial \\ntaxonomy using genomic sequences alone.  \\nThis project shows that useful biological motifs \\ncan be learned by deep models of learning, i.e., \\n1D CNNs, without manual feature engineering \\nand at the same time using traditional ML \\nmodels are comparable in their interpretability \\nand complexity. This comparison shows that \\nmodel choice can be made with respect to the \\nlimitations in computation, and operation \\nresources required for a particular application. \\nVIII. Future Work \\nThere are a number of avenues that can be \\npursued to advance this work: \\n• Multiclass Classification: Moving away from \\nbinary and exploring multiclass models that \\npredict a single bacterial species or genera would \\ngreatly extend the systems clinical and \\necological applicability. \\n• Transformer Architectures: Transformer-based \\nmodels (e.g., DNABERT), pretrained on large \\ngenomic datasets, offer a means to identify \\ndeeper contextual hierarchies and improved \\ngeneralization across taxa. \\n• Unsupervised Pretraining: The application of \\nunsupervised or self-supervised learning \\napproaches (e.g., autoencoders, contrastive \\nlearning) may allow the model to discover more \\ninformative representations on unlabeled DNA. \\n• Real-time and Clinical Deployable: Grounding \\nthe model in a real-time inference pipeline that \\ncould potentially be adapted for additional \\napplications such as point-of-care diagnostics, \\npublic health surveillance, or bioinformatics \\napplications may position the system as a more \\nuseful tool for real-time action. \\n• Ensemble Learning: The investigation of \\nmethods for model combinations aiming to \\nconverge predictions from ML and DL models \\nmay reveal robust classifiers that are more \\ninterpretable.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content=\"By expanding the models functionality and \\napplicability this work lays the groundwork for \\nmore scalable, interpretable, and biologically-\\ninformed microbial classification tools in \\ngenomics. \\nIX. References \\n \\n[1] Alipanahi, B., et al”. 'Predicting the sequence specificities \\nof DNA-and RNA-(binding proteins by deep learning).' \\nNature biotechnology, 2015”. \\n[2] Zou, J., et al.” '(A primer on deep learning in genomics.)'” \\nNature Genetics, 2019. \\n[3] Min, S., et al.[ 'Deep learning in bioinformatics.' Briefings \\nin bioinformatics, ],”2017. \\n[4] Wood, D. E., et al. “'Kraken: ,ultrafast metagenomic \\nsequence classification, using exact alignments.' Genome \\nBiology”], 2014. \\n[5] “Vaswani, A., et al.,,” 'Attention is all you need.”' \\nAdvances in Neural Information Processing Systems, 2017 ,. \\n[6] Du -et al. (2020):” Classification of Chromosomal DNA \\nSequences Using Hybrid Deep Learning Architectures ”.\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Detection of Bacterial Signatures in Genomic DNA Sequences using Hybrid \\nML-DL Pipeline \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbstract \\nProper identification and classification of bacterial life \\nin DNA sequences is one of the central roles of \\nbioinformatics with general implications for clinical \\ndiagnosis, typing of pathogens, characterization of \\nmicrobiome, and genetic research. A fast pace of \\nadvancements in sequencing technologies has amassed \\nvast volumes of genomic data, necessitating robust \\ncomputational approaches to analyze and interpret \\nbiological patterns effectively.Although earlier \\nmachine learning (ML) and deep learning (DL) \\nmodels have been suggested for the task, a more \\ncomprehensive comparative pipeline of both classic \\nML models and DL architectures has not been \\ndeveloped hitherto in this work. The ML approach \\nbegins with rigorous feature engineering, where we \\nobtain biologically meaningful descriptors such as \\nnucleotide composition, k-mer frequency distributions,'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Shannon entropy, and sequence complexity measures. \\nThese features are then employed for the training of \\nthree popular classifiers: Random Forest (RF), \\nSupport Vector Machine (SVM), and Extreme \\nGradient Boosting (XGBoost).At the same time, we \\ndevelop a Convolutional Neural Network (CNN) \\nmodel that processes raw integer-encoded DNA \\nsequences directly without any explicit feature \\nextraction. The CNN model extracts local patterns and \\ndistant dependencies in sequences such that the model \\nlearns deep representations.Our results indicate that all \\nmodels are good at generalizing, with CNN \\ngeneralizing and performing just a little better \\ncompared to the traditional classifiers in terms of noise \\nrobustness. However, the ML models provide better \\ninterpretability, particularly in terms of feature \\nimportance and biological relevance. \\nThis combined system demonstrates the power of \\nbringing together interpretable machine learning and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='the automatic feature learning capabilities of deep \\nlearning, paving the way towards more accurate and \\ninterpretable bacterial DNA classification systems. \\nKeywords: DNA sequence classification, bacterial \\ndetection, bioinformatics, machine learning, deep \\nlearning, convolutional neural network, feature \\nengineering, k-mer frequency, Shannon entropy, \\nRandom Forest, SVM, XGBoost, genomic analysis, \\nexplainable AI. \\nGowtham D  \\nIII M.Sc SS  \\nDepartment of Software Systems And Aiml \\n Sri Krishna Arts And Science College  \\nCoimbatore, India  \\ngowthamd22mss011@skasc.ac.in \\n \\nProf. Sabeena S  \\nDepartment of Software Systems And Aiml \\n Sri Krishna Arts And Science College  \\nCoimbatore, India  \\nsabeenas@skasc.ac.in'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='I. Introduction \\nDNA sequence classification is the ab initio of \\nthe core modern computational biology, helping \\nrecognize a bacterial and viral infection, identify \\ngenetic mutation, and detect certain pathogenic \\nmarkers. All these processes are the very \\nfoundation of disease diagnosis, antimicrobial \\nresistance prediction, personalized medicine, and \\nmicrobiome analysis. As high-throughput \\nsequencing-related technologies evolve in time, \\nthey have to ingest oceanic volumes of genomic \\ndata, thus rendering manual analysis techniques \\ngigantic and slow. \\nTo mitigate these challenges, computational \\nmeans are being deployed to classify DNA \\nsequences automatically. Traditional classical \\nML-based methods-Random forest, SVM, and \\nGradient Boosting-have been shown to work \\nwell for genomics studies. Such models cater to \\nengineered features like nucleotide composition, \\nk-mer frequency profiles, GC content, and \\nentropy-based features that statistically and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='structurally characterize DNA. \\nWhile successful, these ML models are often in \\nneed of handcrafted feature design and tend to \\ndisregard more subtle sequence patterns in the \\nDNA profile. Great DL breakthroughs that \\ncurrently prevail particularly in convolutional \\n \\nII. Related Work  \\n \\nDNA sequence classification employs \\ncomputational methods to be used in \\nbioinformatics for taxonomic classification of an \\norganism, identification of pathogens, or for \\nmetagenomic analysis. The majority of the \\nexisting work employs k-mer-based methods, \\nwherein smaller DNA substrings of a fixed size \\nare extracted from DNA sequences and then used \\nas features in traditional classification schemes. \\nManual features, when used with Naïve Bayes, \\nRandom Forests, and Support Vector Machines, \\nyield the best results. Kraken and CLARK \\nclassify sequences through high-speed exact k-\\nmer searching against reference databases. Both \\nthe methods provide the greatest speed and')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Detection of Bacterial Signatures in Genomic DNA Sequences using Hybrid \\nML-DL Pipeline \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbstract \\nProper identification and classification of bacterial life \\nin DNA sequences is one of the central roles of \\nbioinformatics with general implications for clinical \\ndiagnosis, typing of pathogens, characterization of \\nmicrobiome, and genetic research. A fast pace of \\nadvancements in sequencing technologies has amassed \\nvast volumes of genomic data, necessitating robust \\ncomputational approaches to analyze and interpret \\nbiological patterns effectively.Although earlier \\nmachine learning (ML) and deep learning (DL) \\nmodels have been suggested for the task, a more \\ncomprehensive comparative pipeline of both classic \\nML models and DL architectures has not been \\ndeveloped hitherto in this work. The ML approach \\nbegins with rigorous feature engineering, where we \\nobtain biologically meaningful descriptors such as \\nnucleotide composition, k-mer frequency distributions,'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Shannon entropy, and sequence complexity measures. \\nThese features are then employed for the training of \\nthree popular classifiers: Random Forest (RF), \\nSupport Vector Machine (SVM), and Extreme \\nGradient Boosting (XGBoost).At the same time, we \\ndevelop a Convolutional Neural Network (CNN) \\nmodel that processes raw integer-encoded DNA \\nsequences directly without any explicit feature \\nextraction. The CNN model extracts local patterns and \\ndistant dependencies in sequences such that the model \\nlearns deep representations.Our results indicate that all \\nmodels are good at generalizing, with CNN \\ngeneralizing and performing just a little better \\ncompared to the traditional classifiers in terms of noise \\nrobustness. However, the ML models provide better \\ninterpretability, particularly in terms of feature \\nimportance and biological relevance. \\nThis combined system demonstrates the power of \\nbringing together interpretable machine learning and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='the automatic feature learning capabilities of deep \\nlearning, paving the way towards more accurate and \\ninterpretable bacterial DNA classification systems. \\nKeywords: DNA sequence classification, bacterial \\ndetection, bioinformatics, machine learning, deep \\nlearning, convolutional neural network, feature \\nengineering, k-mer frequency, Shannon entropy, \\nRandom Forest, SVM, XGBoost, genomic analysis, \\nexplainable AI. \\nGowtham D  \\nIII M.Sc SS  \\nDepartment of Software Systems And Aiml \\n Sri Krishna Arts And Science College  \\nCoimbatore, India  \\ngowthamd22mss011@skasc.ac.in \\n \\nProf. Sabeena S  \\nDepartment of Software Systems And Aiml \\n Sri Krishna Arts And Science College  \\nCoimbatore, India  \\nsabeenas@skasc.ac.in'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='I. Introduction \\nDNA sequence classification is the ab initio of \\nthe core modern computational biology, helping \\nrecognize a bacterial and viral infection, identify \\ngenetic mutation, and detect certain pathogenic \\nmarkers. All these processes are the very \\nfoundation of disease diagnosis, antimicrobial \\nresistance prediction, personalized medicine, and \\nmicrobiome analysis. As high-throughput \\nsequencing-related technologies evolve in time, \\nthey have to ingest oceanic volumes of genomic \\ndata, thus rendering manual analysis techniques \\ngigantic and slow. \\nTo mitigate these challenges, computational \\nmeans are being deployed to classify DNA \\nsequences automatically. Traditional classical \\nML-based methods-Random forest, SVM, and \\nGradient Boosting-have been shown to work \\nwell for genomics studies. Such models cater to \\nengineered features like nucleotide composition, \\nk-mer frequency profiles, GC content, and \\nentropy-based features that statistically and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='structurally characterize DNA. \\nWhile successful, these ML models are often in \\nneed of handcrafted feature design and tend to \\ndisregard more subtle sequence patterns in the \\nDNA profile. Great DL breakthroughs that \\ncurrently prevail particularly in convolutional \\n \\nII. Related Work  \\n \\nDNA sequence classification employs \\ncomputational methods to be used in \\nbioinformatics for taxonomic classification of an \\norganism, identification of pathogens, or for \\nmetagenomic analysis. The majority of the \\nexisting work employs k-mer-based methods, \\nwherein smaller DNA substrings of a fixed size \\nare extracted from DNA sequences and then used \\nas features in traditional classification schemes. \\nManual features, when used with Naïve Bayes, \\nRandom Forests, and Support Vector Machines, \\nyield the best results. Kraken and CLARK \\nclassify sequences through high-speed exact k-\\nmer searching against reference databases. Both \\nthe methods provide the greatest speed and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='accuracy with familiar organisms but do not have \\ngreat prowess in unknown taxa because of heavy \\ndependency on reference genomes and \\nincapability of flexing toward some new \\ngenomic patterns. With the emergence of deep \\nlearning, the new possibilities arose for sequence \\nclassification. The RNNs and CNNs \\nautomatically learn hierarchical features from \\nraw RNA or DNA sequences without requiring'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='manual feature engineering. Thus, an example \\nwould be DeepSEI \\nIII. Dataset Description \\nThe data in use are from a Kaggle competition \\ntitled \"3722genomics\", which classifies DNA \\nsequences for the identification of a bacterial \\npresence. Each sample in the data carries its \\nunique id, a DNA sequence, and a binary tag (Y) \\nin which 1 indicates bacterial presence while 0 \\nmeans bacterial absence. The data are distributed \\ninto a labeled training set and an unlabeled test \\nset containing 100,000 and 20,000 sequences \\nrespectively. \\nThe class balance in the training dataset is nearly \\neven, with 50,063 of the negative (no bacteria) \\nclass instances and 49,937 of the positive (where \\nbacteria are present) class instances. The \\nbalanced representation prevents the model from \\nfavoring one class over another and equally \\naccounts for performance measurement. \\nDNA sequences vary in length in this data set, \\nthus replicating an actual situation within'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='genomic classification problems. Sequence \\nlength is a random variable between 121 and 425 \\nbase pairs. The distribution of lengths is \\npresented with a mean of roughly 268 base pairs \\nand a standard deviation of 77.08. The \\ninterquartile range shows that 25% of the \\nsequences are shorter than 202 base pairs, while \\n75% \\nFigure 1: Count Plot for Target Distribution. \\n \\n \\nFigure 2: DNA Sequence Distribution. \\n \\nIV. Methodology \\n4.3.1 Random Forest Classifier \\nRandom Forest is an ensemble learning approach \\nbased on decision trees. Its process begins by \\ntraining a considerable amount of trees \\nsimultaneously, and when classifying it takes a \\nmajority vote of all the trees. Random Forest is a \\nrobust, non-parametric machine learning \\napproach suitable for high dimensional feature \\nspaces with relatively low risk of overfitting \\nwhen applying noisey bio data.  \\nRandom Forest was selected due to the \\ninterpretable nature of its predictions, and the \\ninherent feature importance score that'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='complements our process for selecting variables. \\nRandom Forest used the default hyperparameters \\nand was implemented with 100 estimators \\nTable 1. Performance metrics of RF \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.97 0.98 9,987 \\n \\nFigure 3: Confusion matrix for Random Forest \\nclassifier. \\n \\n \\n4.3.2 Support Vector Machine (SVM) \\nSupport Vector Machines depend on finding the \\nhyperplane that optimally separates the data into \\nclasses, a non-linear example of which can be \\naddressed using kernel tricks, such as RBF, \\nwhich will map the input features into higher \\ndimensions where the data is linearly separable. \\nSVM is most powerful in high-dimensions, and \\nit is also very well known for its properties of \\ngeneralization. \\nSince we are dealing with features from DNA \\nsequences, which will likely be non-linearly \\nseparable, we applied the RBF kernel in our \\ntrials. The regularization parameter C was set to \\nits default value in order to balance having a'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='small error on the training data and a large \\nmargin. \\n \\n \\nTable 2. Performance metrics of SVM \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.98 0.98 9,987 \\n \\nFigure 4: Confusion matrix for Support Vector \\nMachine.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='4.3.3 XGBoost Classifier \\nXGBoost or Extreme Gradient Boosting is a very \\nfast regularized boosting algorithm that performs \\nvery well on structured data problems. XGBoost \\nbuilds decision trees sequentially, so one tree \\nshould help minimize the errors from the \\nprevious trees. It also has regularization terms in \\nthe objective function to avoid overfitting.  \\nWe chose XGBoost because it can also \\nefficiently work on sparse high-dimensional \\ndata, and it also has built-in parallelization in the \\ntraining which makes computation faster. Some \\nof the parameters we used were max_depth=6, \\nn_estimators=100 and learning_rate=0.1. \\nTable 3. Performance metrics of XGB \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.98 0.98 9,987 \\n \\nFigure 5: Confusion matrix for XGBoost \\nclassifier. \\n \\n \\n4.3.4 Model comparison and takeaways \\nAll models performed nearly the same with 98% \\noverall accuracy, suggesting biological inspired \\nfeatures are good predictors. Although SVM and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='XGBoost had slightly higher precision and recall \\nthan Random Forest those differences were \\nnegligible. All confusion matrices among the \\nmodels suggest balanced classification with very \\nlittle misclassification which shows the feature \\nselection and preprocessing pipeline were best \\nmatched to the classification problem.  \\nA radar plot or side by side comparative bar \\nchart of the precision, recall and F1-score for all \\nmodels could be included to view these metrics \\nin a side by side manner. \\nFigure 6: Model performance comparison \\nacross Random Forest, SVM, and XGBoost.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='4.3 Deep Learning with 1D Convolutional \\nNeural Network (CNN) \\nTo eliminate manual feature extraction and allow \\nthe model to learn directly from the genomic \\nsequences in its raw form, a 1D Convolutional \\nNeural Network (CNN) was used. This structure \\nis particularly appropriate for sequential data and \\nhas performed exceedingly well in applications \\ninvolving natural language and biological \\nsequences. The key advantage of CNNs is that \\nthey perform localized pattern and motif finding \\nthrough hierarchical feature processing. \\nSequence Encoding and Preprocessing \\nAll DNA sequences in the dataset are made up of \\ncombinations of the four nucleotide bases: \\nAdenine (A), Cytosine (C), Guanine (G), and \\nThymine (T). Initially, they were encoded as \\nintegers: A = 0, C = 1, G = 2, T = 3. Because \\nsequences vary in length in the dataset, the \\nshorter sequences were padded with a specific \\ninteger token (PAD = 4) to a uniform length. \\nSequences were padded or cut-off from their end'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content=\"to the maximum length of 425 bases depending \\non their lengths' statistical distributions. The \\npreprocessing did ensure uniformity but had no \\nadverse affect on biologically important \\ninformation. \\nCNN Model Architecture \\nThe CNN model was developed to learn high-\\nlevel features from the integer-encoded genomic \\nDNA sequences. This architecture has several \\nlayers: \\n•Embedding Layer: This layer converts the \\ninteger encodings of the nucleotide bases to \\ndense vector embeddings. A base embedding \\nbecomes a learnable embedding, and thus the \\nmodel learns the semantic similarity of \\nnucleotides in the context of the genome. \\n•Convolutional Layers: Several 1D \\nconvolutional layers of different kernel sizes \\n(i.e., 5, 7) to learn local patterns or motifs in the \\nsequence. These motifs will typically denote \\nfunctional sites in the DNA that have meaningful \\nroles in classification. \\n•Max-Pooling Layers: Following every \\nconvolution block is a max-pooling layer to\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='decrease the dimensions of the feature maps. \\nThis not only maximizes computation efficiency, \\nbut allows for spatial invariance; keeping only \\nthe most salient features, thus preserving only \\nnecessary information. \\n•Flatten Layer: The output of the last pooling \\noperation is then flattened into a one-\\ndimensional vector for classification utilizing \\ndense layers.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='•Dense Layers with Dropout: Fully connected \\nlayers one or more are used, with ReLU \\nactivations to build in non-linearity. Dropout \\nregularization is used to reduce overfitting by \\nrandomly disabling a number of neurons during \\ntraining.  \\n•Output Layer: The output dense layer uses a \\nsigmoid activation in order to produce \\nprobability scores for binary classification or, \\nmore specifically, the predicted probability of \\nbacteria present (label = 1).  \\n•Optimization and Loss: The model is \\noptimized with the binary cross entropy loss \\nfunction and the adam optimizer. Adams \\nadaptive learning rate allows it to converge more \\nquickly and generalize better.  \\nTraining Strategy and Monitoring \\nThe dataset was split into a training set and a \\nvalidation set, which the model performed \\nevaluation on while training. The validation \\naccuracy and loss were recorded over the epochs \\nof training, and an early stopping criterion was \\nadded to allow the model to stop training when it'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='stopped improving. This will help prevent \\noverfitting and allow the model to generalize to \\nunseen test data better. \\n \\n \\n \\n \\nFigure 7: Architecture of the 1D Convolutional \\nNeural Network used for genomic sequence \\nclassification.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Model Evaluation \\nThe CNN was evaluated on the test set, which \\ncontained 20 thousand separate unseen DNA \\nsequences. The results indicated a robust level of \\nclassification performance, achieving high scores \\n(precision, recall and F1-scores) across both \\nclasses. Below is the full classification report: \\nTable 4. Performance metrics of CNN \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n0 0.98 0.99 0.98 10,013 \\n1 0.99 0.98 0.98 9,987 \\nOverall these results show that the model is able \\nto appropriately discriminate between samples \\nthat are bacterial and non-bacterial, performing \\nsimilarly across both classes. \\nFigure 8: Confusion matrix showing true and \\npredicted labels for the CNN mode. \\n \\nThe confusion matrix suggests that the \\nmislabeled instances are relatively small, and the \\nmajority of the predictions were correctly \\naligned with the ground truth labels. \\n \\nV. Results \\n5.1 Evaluation Metrics \\nFor evaluating the performance of our models,'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='we used standard classification metrics: \\naccuracy, precision, recall, and F1-score. All \\nevaluations were also completed on a stratified \\nvalidation set to ensure a balanced representation \\nof each class. \\nThe Random Forest and XGBoost classifiers \\nperformed similarly well, both with F1-scores \\nover 0.98, with XGBoost doing slightly better in \\nprecision and recall measures. The Support \\nVector Machine (SVM) model performed \\nadequately, but had generally lower outcomes \\nthan the other classifiers with regards to F1- \\nscores and accuracy. \\nThe best performing 1D Convolutional Neural \\nNetwork (CNN) architecture yielded the best F1-\\nscore of 0.989, with excellent generalization \\ndemonstrated across the complete dataset, as \\ndemonstrated through macro and weighted \\naverage metrics. These results suggest that deep \\nlearning models exhibit marginally better \\npredictive performance than traditional machine \\nlearning models when tasked with DNA'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='sequence classification tasks, albeit at the cost of \\ninterpretability.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Table 5. Performance metrics comparison \\nacross models \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\nRF 0.981 0.982 0.981 0.981 \\nXGB 0.984 0.985 0.983 0.984 \\nSVM 0.974 0.976 0.973 0.974 \\nCNN \\n(1D) \\n0.989 0.99 0.988 0.989 \\n \\n5.2 Visualization and Interpretability \\nTo improve interpretation and to facilitate \\nperformance comparison between models, we \\nwill plot some key evaluation areas: \\n• A bar plot of F1-scores for all the models for a \\nrelative comparison of model performance. \\n• A plot of feature importance of the Random \\nForest model that includes the top 10 \\ncontributing features. These features are high-\\nfrequency k-mers and compositional features \\nsuch as GC content that have biological meaning \\nin taxonomic classification. \\n• The training history charts for the CNN model \\n(training and validation accuracy and loss vs \\nepochs), confirm that the model converged \\nwithout overfitting (i.e., demonstrating strong \\nlearning with the raw DNA sequences).'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Figure 9: F1-score comparison across ML and \\nDL models. \\n \\nFigure 10: Feature importance plot of top 10 \\nfeatures from Random Forest model. \\n \\nFigure 11: CNN training history showing \\naccuracy and loss over epochs.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content=\"5.3 Model Insights and Comparative Analysis \\nThe evaluation offers significant insights into the \\nadvantages and trade-offs of all approaches: \\n• CNN models are very good at examining \\nsequence motifs directly from raw DNA, with no \\nfeature engineering necessary by a human. The \\nmodel's overall best, especially on bigger \\ndatasets, demonstrates a good ability to identify \\ncomplex biological patterns. \\n• Machine Learning models, while slightly \\ntrailing in raw performance, have significantly \\nbetter interpretability and computational cost. \\nThe feature importance outputs of Random \\nForest and XGBoost not only provide \\nexplainability but also begin to show us how to \\nthink about the sequence patterns at a biological \\nlevel- like how GC-rich motifs are usually in the \\ngenomes of microbes. \\n• This hybrid evaluation also emphasizes the \\nneed for the combination of ML explainability \\nwith DL accuracy, especially in fields where \\nperformance and explainability are most \\nimportant.\"),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='important. \\nOverall, our comparative work illustrates that \\ntraditional and deep learning methods have \\ninherent strengths to be considered and ought to \\nbe chosen based on the demands of the specific \\nbiological application. \\nVI. Discussion \\nThe results of this work demonstrate the \\ncomplementary strengths of the state-of-the-art \\nMachine Learning (ML) and Deep Learning \\n(DL) approaches, for the case of bacterial \\nclassification from genomic data. The \\nperformance of traditional ML models, i.e., \\nRandom Forest, XGBoost and Support Vector \\nMachines, is reasonable with hand-engineered \\nfeatures such as k-mer frequencies and \\ncompositional statistics. Traditional models have \\nadvantages of low computational overhead, fast \\ninference, and good interpretability making them \\napplicable in real-time or low-resource \\ndeployment. In contrast, our 1D Convolutional \\nNeural Network (CNN) offered modestly better \\npredictive accuracy and F1-score compared to'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='traditional models, and could learn from raw \\nDNA sequence data. CNNs are capable of \\nidentifying subtle, complex, non-linear motifs, \\nthat may be difficult to hand-engineer, and are \\nparticularly useful in noisy or heterogeneous \\ngenomic data. But come with greater \\ncomputational overhead, longer training time and \\nweaker interpretability.  For many use cases such \\nas large-scale genomic surveillance, \\nmetagenomic classification or auto-diagnosis, \\nDL approaches offer improved scalability and'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='flexibility to deal with unknown data. \\nOne of the promising avenues is hybrid or \\nensemble approaches, which leverage the \\ninterpretability of ML and combine it with the \\nDL representation learning capability. The \\nensembles can apply voting or stacking methods \\nto further improve classification accuracy with \\ntransparency from the model. \\nVII. Conclusion  \\nWe report here a complete pipeline for \\nclassifying bacteria by using DNA sequences in \\nconcert with feature-engineering machine \\nlearning models, and end-to-end deep learning \\nmodels.  \\nOur approach performs very well for \\nclassification, with all models producing \\nclassification F1-scores > 0.97 and the CNN \\nproducing an F1-score of 0.989, demonstrating \\nthat it is possible to automate microbial \\ntaxonomy using genomic sequences alone.  \\nThis project shows that useful biological motifs \\ncan be learned by deep models of learning, i.e., \\n1D CNNs, without manual feature engineering \\nand at the same time using traditional ML'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='models are comparable in their interpretability \\nand complexity. This comparison shows that \\nmodel choice can be made with respect to the \\nlimitations in computation, and operation \\nresources required for a particular application. \\nVIII. Future Work \\nThere are a number of avenues that can be \\npursued to advance this work: \\n• Multiclass Classification: Moving away from \\nbinary and exploring multiclass models that \\npredict a single bacterial species or genera would \\ngreatly extend the systems clinical and \\necological applicability. \\n• Transformer Architectures: Transformer-based \\nmodels (e.g., DNABERT), pretrained on large \\ngenomic datasets, offer a means to identify \\ndeeper contextual hierarchies and improved \\ngeneralization across taxa. \\n• Unsupervised Pretraining: The application of \\nunsupervised or self-supervised learning \\napproaches (e.g., autoencoders, contrastive \\nlearning) may allow the model to discover more \\ninformative representations on unlabeled DNA.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='• Real-time and Clinical Deployable: Grounding \\nthe model in a real-time inference pipeline that \\ncould potentially be adapted for additional \\napplications such as point-of-care diagnostics, \\npublic health surveillance, or bioinformatics \\napplications may position the system as a more \\nuseful tool for real-time action. \\n• Ensemble Learning: The investigation of \\nmethods for model combinations aiming to \\nconverge predictions from ML and DL models \\nmay reveal robust classifiers that are more \\ninterpretable.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content=\"By expanding the models functionality and \\napplicability this work lays the groundwork for \\nmore scalable, interpretable, and biologically-\\ninformed microbial classification tools in \\ngenomics. \\nIX. References \\n \\n[1] Alipanahi, B., et al”. 'Predicting the sequence specificities \\nof DNA-and RNA-(binding proteins by deep learning).' \\nNature biotechnology, 2015”. \\n[2] Zou, J., et al.” '(A primer on deep learning in genomics.)'” \\nNature Genetics, 2019. \\n[3] Min, S., et al.[ 'Deep learning in bioinformatics.' Briefings \\nin bioinformatics, ],”2017. \\n[4] Wood, D. E., et al. “'Kraken: ,ultrafast metagenomic \\nsequence classification, using exact alignments.' Genome \\nBiology”], 2014. \\n[5] “Vaswani, A., et al.,,” 'Attention is all you need.”' \\nAdvances in Neural Information Processing Systems, 2017 ,. \\n[6] Du -et al. (2020):” Classification of Chromosomal DNA \\nSequences Using Hybrid Deep Learning Architectures ”.\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Science\\LangChain\\GenAi_BotGemma\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize Gemini embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\"   # ✅ Gemini embedding model\n",
    ")\n",
    "db=FAISS.from_documents(documents,embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.3 XGBoost Classifier \\nXGBoost or Extreme Gradient Boosting is a very \\nfast regularized boosting algorithm that performs \\nvery well on structured data problems. XGBoost \\nbuilds decision trees sequentially, so one tree \\nshould help minimize the errors from the \\nprevious trees. It also has regularization terms in \\nthe objective function to avoid overfitting.  \\nWe chose XGBoost because it can also \\nefficiently work on sparse high-dimensional \\ndata, and it also has built-in parallelization in the \\ntraining which makes computation faster. Some \\nof the parameters we used were max_depth=6, \\nn_estimators=100 and learning_rate=0.1. \\nTable 3. Performance metrics of XGB \\nClass  Precision  Recall  F1-\\nScore  \\nSupport  \\n1 0.99 0.98 0.98 9,987 \\n \\nFigure 5: Confusion matrix for XGBoost \\nclassifier. \\n \\n \\n4.3.4 Model comparison and takeaways \\nAll models performed nearly the same with 98% \\noverall accuracy, suggesting biological inspired \\nfeatures are good predictors. Although SVM and'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"What are the ML algorithms used in this \"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaLLM\n",
    "\n",
    "# llm= OllamaLLM(model=\"gemma\")\n",
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided conext\n",
    "Think step by step before providing a detailed answer\n",
    "I'll tip you $1000 if the user finds the answer helpful\n",
    "<context>{context}</context>\n",
    "Question: {input}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_Chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D78D36FE00>, search_kwargs={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain=create_retrieval_chain(retriever,document_Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response=retriever_chain.invoke({\"input\":\"\"\" The CNN model extracts local patterns and\n",
    "# distant dependencies in sequences such that the model\n",
    "# learns deep representations\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Correct env variable\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    google_api_key=api_key,   \n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retriever_chain.invoke({\"input\":\"\"\" The CNN model extracts local patterns and\n",
    "distant dependencies in sequences such that the model\n",
    "learns deep representations\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ' The CNN model extracts local patterns and\\ndistant dependencies in sequences such that the model\\nlearns deep representations',\n",
       " 'context': [Document(id='ce74a3d0-fd2e-4948-948f-bc337db29b12', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='4.3 Deep Learning with 1D Convolutional \\nNeural Network (CNN) \\nTo eliminate manual feature extraction and allow \\nthe model to learn directly from the genomic \\nsequences in its raw form, a 1D Convolutional \\nNeural Network (CNN) was used. This structure \\nis particularly appropriate for sequential data and \\nhas performed exceedingly well in applications \\ninvolving natural language and biological \\nsequences. The key advantage of CNNs is that \\nthey perform localized pattern and motif finding \\nthrough hierarchical feature processing. \\nSequence Encoding and Preprocessing \\nAll DNA sequences in the dataset are made up of \\ncombinations of the four nucleotide bases: \\nAdenine (A), Cytosine (C), Guanine (G), and \\nThymine (T). Initially, they were encoded as \\nintegers: A = 0, C = 1, G = 2, T = 3. Because \\nsequences vary in length in the dataset, the \\nshorter sequences were padded with a specific \\ninteger token (PAD = 4) to a uniform length. \\nSequences were padded or cut-off from their end'),\n",
       "  Document(id='e0b1f952-56f2-4654-98dc-3e580dcc9852', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Shannon entropy, and sequence complexity measures. \\nThese features are then employed for the training of \\nthree popular classifiers: Random Forest (RF), \\nSupport Vector Machine (SVM), and Extreme \\nGradient Boosting (XGBoost).At the same time, we \\ndevelop a Convolutional Neural Network (CNN) \\nmodel that processes raw integer-encoded DNA \\nsequences directly without any explicit feature \\nextraction. The CNN model extracts local patterns and \\ndistant dependencies in sequences such that the model \\nlearns deep representations.Our results indicate that all \\nmodels are good at generalizing, with CNN \\ngeneralizing and performing just a little better \\ncompared to the traditional classifiers in terms of noise \\nrobustness. However, the ML models provide better \\ninterpretability, particularly in terms of feature \\nimportance and biological relevance. \\nThis combined system demonstrates the power of \\nbringing together interpretable machine learning and'),\n",
       "  Document(id='149362f0-2efd-424f-acb6-11554150daf6', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='traditional models, and could learn from raw \\nDNA sequence data. CNNs are capable of \\nidentifying subtle, complex, non-linear motifs, \\nthat may be difficult to hand-engineer, and are \\nparticularly useful in noisy or heterogeneous \\ngenomic data. But come with greater \\ncomputational overhead, longer training time and \\nweaker interpretability.  For many use cases such \\nas large-scale genomic surveillance, \\nmetagenomic classification or auto-diagnosis, \\nDL approaches offer improved scalability and'),\n",
       "  Document(id='cb0b8a84-8da7-4430-b01d-a3c200e5e154', metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-09-03T07:41:29+00:00', 'author': 'python-docx', 'moddate': '2025-09-03T07:41:29+00:00', 'source': 'DNA_Bacteria_Signature using Genomic DNA Sequencing.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content=\"to the maximum length of 425 bases depending \\non their lengths' statistical distributions. The \\npreprocessing did ensure uniformity but had no \\nadverse affect on biologically important \\ninformation. \\nCNN Model Architecture \\nThe CNN model was developed to learn high-\\nlevel features from the integer-encoded genomic \\nDNA sequences. This architecture has several \\nlayers: \\n•Embedding Layer: This layer converts the \\ninteger encodings of the nucleotide bases to \\ndense vector embeddings. A base embedding \\nbecomes a learnable embedding, and thus the \\nmodel learns the semantic similarity of \\nnucleotides in the context of the genome. \\n•Convolutional Layers: Several 1D \\nconvolutional layers of different kernel sizes \\n(i.e., 5, 7) to learn local patterns or motifs in the \\nsequence. These motifs will typically denote \\nfunctional sites in the DNA that have meaningful \\nroles in classification. \\n•Max-Pooling Layers: Following every \\nconvolution block is a max-pooling layer to\")],\n",
       " 'answer': 'Based on the provided context, here is a step-by-step answer to your question.\\n\\n**Step 1: Identify the core statement.**\\nThe statement provided is: \"The CNN model extracts local patterns and distant dependencies in sequences such that the model learns deep representations\".\\n\\n**Step 2: Scan the context for the exact or similar phrasing.**\\nThe provided text is searched for the keywords \"CNN model\", \"local patterns\", \"distant dependencies\", and \"deep representations\".\\n\\n**Step 3: Locate the relevant sentence in the context.**\\nThe context contains the exact sentence you provided. It is located in the second column of the text:\\n\\n> \"The CNN model extracts local patterns and distant dependencies in sequences such that the model learns deep representations.\"\\n\\n**Step 4: Synthesize the answer based *only* on the provided information.**\\nThe statement is a direct quote from the provided text. The context further supports this statement by mentioning related capabilities of the CNN model:\\n*   The key advantage of CNNs is that they perform \"localized pattern and motif finding through hierarchical feature processing.\"\\n*   CNNs are described as being \"capable of identifying subtle, complex, non-linear motifs\".\\n*   The model was developed to \"learn high-level features from the integer-encoded genomic DNA sequences.\"\\n\\n**Conclusion:**\\n\\nYes, according to the provided context, the statement is correct. The text explicitly states: **\"The CNN model extracts local patterns and distant dependencies in sequences such that the model learns deep representations.\"**'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
